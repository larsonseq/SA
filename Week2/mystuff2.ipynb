{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a9337d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d01d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix \n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./hacktrain.csv\")\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df to avoid modifying ID and class columns\n",
    "df_numeric = df.select_dtypes(include='number').copy()\n",
    "\n",
    "# Group by 'class' and fill NaNs with group mean using transform\n",
    "df_numeric_imputed = df.groupby('class')[df_numeric.columns].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Assign imputed numeric columns back to original df\n",
    "df[df_numeric.columns] = df_numeric_imputed\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff567be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993ad01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum(), '\\n\\n')\n",
    "\n",
    "# print(df[df.duplicated()])  # it is 0\n",
    "plt.figure(figsize=(17, 12))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50130ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27a616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ad47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a subset of the DataFrame excluding 'class' and 'ID' columns\n",
    "data = df.drop(columns=['class', 'ID'])\n",
    "\n",
    "# Step 2: Calculate the correlation matrix on the subset DataFrame\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Step 3: Plot the heatmap to visualize correlations\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Set a threshold for correlation (e.g., 0.9)\n",
    "threshold = 0.9\n",
    "\n",
    "# Step 5: Identify columns to drop by checking pairwise correlations\n",
    "drop_columns = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:  # if correlation is above the threshold\n",
    "            colname = corr_matrix.columns[i]\n",
    "            drop_columns.add(colname)\n",
    "\n",
    "# Step 6: Drop the identified collinear columns from the original 'data' DataFrame\n",
    "data_cleaned = data.drop(columns=drop_columns)\n",
    "\n",
    "# Step 7: Re-merge the 'class' and 'ID' columns back into the cleaned DataFrame\n",
    "df_cleaned = pd.concat([df[['class', 'ID']], data_cleaned], axis=1)\n",
    "\n",
    "# Step 8: Show remaining columns after cleaning\n",
    "print(\"Remaining columns after removing collinear ones:\")\n",
    "print(df_cleaned.columns)\n",
    "\n",
    "df = df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6e115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365d85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load original (non-imputed) data\n",
    "df_original = pd.read_csv('hacktrain.csv')  # Replace with your actual path\n",
    "\n",
    "# Step 2: `df` is your imputed DataFrame (already in memory)\n",
    "\n",
    "# Step 3: Summary of statistics (unchanged)\n",
    "summary = pd.DataFrame({\n",
    "    'Original_Mean': df_original.mean(numeric_only=True),\n",
    "    'Imputed_Mean': df.mean(numeric_only=True),\n",
    "    'Mean_Diff': df.mean(numeric_only=True) - df_original.mean(numeric_only=True),\n",
    "    'Original_Std': df_original.std(numeric_only=True),\n",
    "    'Imputed_Std': df.std(numeric_only=True),\n",
    "    'Std_Diff': df.std(numeric_only=True) - df_original.std(numeric_only=True)\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n=== Summary of Statistical Differences (Original vs Imputed) ===\\n\")\n",
    "print(summary)\n",
    "\n",
    "# Step 4: KDE Plots in 3 Batches with consistent colors\n",
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "batch_size = 10\n",
    "\n",
    "# Define color palettes to alternate\n",
    "palettes = ['Set1', 'Set2', 'Paired']\n",
    "\n",
    "for batch_idx, i in enumerate(range(0, len(numeric_cols), batch_size)):\n",
    "    batch_cols = numeric_cols[i:i + batch_size]\n",
    "    palette = sns.color_palette(palettes[batch_idx % len(palettes)], n_colors=len(batch_cols))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for col, color in zip(batch_cols, palette):\n",
    "        # Original with dashed line\n",
    "        sns.kdeplot(df_original[col], label=f'{col} - Original', linestyle='--', color=color)\n",
    "        # Imputed with solid line\n",
    "        sns.kdeplot(df[col], label=f'{col} - Imputed', linestyle='-', color=color)\n",
    "\n",
    "    plt.title(f'KDE Comparison (Original vs Imputed) — Columns {i+1} to {i+len(batch_cols)}', fontsize=14)\n",
    "    plt.legend(ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd11a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ————————————————————\n",
    "# 1. Load & Prep\n",
    "# ————————————————————\n",
    "df = df.drop(columns=['Unnamed: 0', 'ID'], errors='ignore')\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['class'])\n",
    "X = df.drop(columns=['class'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ————————————————————\n",
    "# 2. Build Pipeline with Degree 2 PolynomialFeatures\n",
    "# ————————————————————\n",
    "pipe = Pipeline([ \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('clf', LogisticRegression(\n",
    "        solver='saga',\n",
    "        penalty='l2',\n",
    "        C=15,\n",
    "        class_weight=None,\n",
    "        max_iter=1000,\n",
    "        multi_class='multinomial',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ————————————————————\n",
    "# 3. Train & Evaluate\n",
    "# ————————————————————\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print({\n",
    "    \"clf__solver\": \"saga\",\n",
    "    \"clf__penalty\": \"l2\",\n",
    "    \"clf__class_weight\": None,\n",
    "    \"clf__C\": 15\n",
    "})\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"hacktest.csv\")\n",
    "test_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test_data.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID=test_data['ID']\n",
    "test_data.drop(['ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[[col for col in test_data.columns if col in df.columns and col != 'class']]\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decoded = le.inverse_transform(y_test)\n",
    "y_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\n",
    "    'ID': ID,\n",
    "    'class': y_decoded\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"submission_mystuff2v1.csv\", index=False) #this file will appear under the output section of the right navbar. You need to submit this csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
